\chapter{Intelligent and Multi Agent Coverage Policies}

The coverage policies described in the last chapter make use of A-Star, an algorithm often classified as an example of 'artificial intelligence'. However, those policies attempt to work by following very simple rules most of the time, and A-Star is used only as a last resort. These policies are easy to understand due to their simplicity, and the adaptability of A-Star along with guarantees about properties of the environment make it easy to feel confident in their correctness. However, these policies may miss significant opportunities for behavior optimization in certain kinds of environment. In addition, the policies of the previous chapter did nothing to take advantage of the multi agent setting in which they operate. While the drone dropout and reintroduction feature introduced in this chapter will give these policies some redeeming properties for the multi agent case, more sophisticated approaches are necessary to take full advantage of the power of a team of covering robots.

\section{The Clustering Low Policy}
%K Means Low Policy in the code!

The first of these policies is the Clustering Low Policy (CLP). This policy manages a team of multiple drones by giving each of them a portion of the overall coverage space to complete. Because it is not straightforward to compute an optimal partitioning of an arbitrary coverage space between a team of covering robots, this policy uses an iterative clustering technique to come up with a reasonable approximation of this division. In addition, because this is the first of several policies designed to adapt to unforeseeable events such as drone dropout and reintroduction, this clustering operation is repeated throughout the coverage scenario. By adaptively redistributing assigned territory online, the CLP algorithm offers a single elegant solution to the dual problems of unforeseeable events and the difficulty of computing an optimal territory partition in one step.

\subsection{K Means Clustering}

The algorithm used to determine and change the division of territory assigned to each drone is a lightly modified variant of the K Means Clustering (KMC) algorithm. KMC was developed by \citeauthor{KMC} in 1957 and published in its standard form in 1982 \cite{KMC}. As it relates to this problem, the goal of KMC is to find some number $k$ of 'means' for a collection of data points such that the average squared distance from any point to the nearest mean is minimized. Once these means have been discovered, the original points can be divided into $k$ groups according to which of the $k$ means they are closest to.

The optimal solution to the problem of K Means Clustering is NP-Hard in general, so there is not a computationally efficient way to solve this problem perfectly. There are several algorithms which do a good job of approximating the solution to this problem with various claims to near-optimality and runtime. One of the most popular such algorithms and the one used in this work is the variant developed by \citeauthor{KMC}, sometimes known as 'naive k means'.

This variant of the algorithm is best known perhaps because of its simplicity. Given an initialization, possibly random, of $k$ means for some value of k, this algorithm simply repeats two steps. It assigns each point in the space to whichever mean is currently closest, then it replaces each mean with the centroid of the points assigned to it. This causes the location of the means to jump through the space with each iteration, gradually converging on a local minimum to the implicit cost function. It is common to simply run this algorithm until there is no change between two iterations, after which point the algorithm will make no further progress and terminates.

While Naive KMC is not the most computationally efficient way to perform K Means Clustering with a comparable level of optimality, it is more than adequate for this application. The number of points being divided into clusters tends to be small, and experimental runtime profiling suggests that these computations take a very small fraction of the overall time spent running simulations.

The exact implementation of K Means Clustering used in this work varies only slightly from the generic version described above. First, the mean of a collection of positions is always rounded into a discrete position value. This is because these means supply a possible location for the corresponding drone to be commanded to. In addition, because this variant of K Means Clustering will be run repeatedly on a smaller and smaller set of positions to be covered, it must eventually handle a case where some of the means actually have no positions associated with them. In this case, each drone gets a random uncovered position assigned to it. Finally, most runs of this K Means Variant use just one or a few iterations. This is because the previously described unconventional setting in which K Means CLustering is run can lead to issues with convergence. In addition, it is not necessary for a drone to have the optimally computed cluster before making any moves. Instead, four iterations of this algorithm are run at the start of most scenarios involving this or other policies that use clustering. This is enough to give each drone a distinct section of territory to pursue, and the particulars of the Clustering Low Policy, discussed below, ensure that this clustering algorithm continues to be used appropriately through the end of the scenario.

In other words, the use of k means clustering results in a partitioning of the space containing the data points into Voronoi cells. Because the algorithm penalizes clusters with large spatial extents quadratically, the Voronoi cells produced by K Means Clustering tend to be of comparable spatial extent to each other. Because of this property, using this algorithm to divide territory between drones tends to give each one a comparable amount of space to cover. This does not guarantee that each drone will require the same amount of time to cover its assigned territory, but it provides a reasonable first guess.

\subsection{Details of the Clustering Low Policy}

With the details of this customized K Means Clustering algorithm in place, it is now possible to describe the details of how the Clustering Low Policy commands moves. The CLP maintains a data structure that, among other things, associates a 'mean' position and a set of territory in the environment with each drone. Every time nextMove is called on an instance of this policy, these means and information about the environment information observed so far are passed to a function that sets up a new iteration of K Means Clustering. In this iteration, the set of locations to be divided and assigned to each mean is generated by reviewing which locations have not been completely covered.

Note that the details of this criteria can very in this project's other uses of the K Means Clustering algorithm. For example, some coverage policies that use KMC to command drones flying at a high altitude, discussed shortly, pass a function that will keep only completely unobserved locations in the set of points to be divided between means.

Once this information has been passed along, KMC returns a new set of means and territory assignments associated with each drone. Next, directions for each drone are computed. Each drone that is not already in the process of seeking a particular location is given a new location to seek. This location is selected from the most recently computed territory associated with the given drone. Within this set of possibilities, CLP attempts to select the location that is farthest from any other drone's mean position. Based on this assignment, a new sequence of movements is computed that will bring the drone from its current position to that newly assigned location. As in other policies, this sequence of moves uses A-Star search. The particular cost function used by A-Star applies a penalty to moves that would cause a drone to cover previously explored locations, and this penalty is especially effective at optimizing the paths created for the CLP.

The decision to make each drone seek out whatever position is farthest from all other means is a heuristic that attempts to have each drone explore the section of its territory that is most likely to be better suited to exploration by that drone than by any other. While this heuristic does not make optimal decisions with respect to that criteria, and while it is not clear that sequencing of positions to visit by such a heuristic will be useful, experimental results show that this policy works well compared to the simple sweep policies.

The rationale for assigning these 'middle-of-territory' positions to be visited first is that if another drone ends up needing more territory to explore, this heuristic ensures that the patches closest to that other drone are likely to go unexplored until that territory reassignment occurs. Likewise, if a drone exploring its own territory ends up needing to take on more work, the fact that other drones are behaving according to this heuristic makes it likely that more territory near where the drone is exploring will be available to take over. In addition, a well tuned choice of penalty function for A-Star path planning seems to handle most of the issues with how commanded locations are sequenced: many locations that should have been visited before the currently commanded one end up on the path generated by A-Star. Some of these descriptions of good behavior break down slightly as the number of locations to be covered dwindles to just a few per drone, but the bulk of the coverage task is handled well by this policy.

\section{A Single Agent Spanning Tree Policy}

As discussed in chapter one, many robotic coverage algorithms have managed to achieve good performance through methods that compute spanning trees on a modified graph of the environment. In this thesis, the Low Spanning Tree Policy (LSTP) is a policy that incorporates these improvements for the environments in \textit{oprc\_env}. LSTP, like the policies in chapter 3, is primarliy a single agent policy. While there are several more sophisticated multi agent spanning tree based policies developed for this setting, LSTP provides the most direct point of comparison to the other single agent policies. This makes it possible to analyze the effect of introducing the superior planning capabilities of this policy without having to consider the effects of the other policy changes. While these additional changes produce policies that tend to have faster overall coverage performance, the single agent policies tend to have some of the best results when it comes to the efficient utilization of individual drones. Before describing the Low Spanning Tree Policy itself, however, it is necessary to discuss the spanning tree generation algorithm that this and other policies rely on.

\subsection{Depth First Spanning Tree Generation and Path Generation Algorithms}

The spanning tree algorithm used by this policy generates a spanning forest, or a collection of spanning trees, using a depth first search based algorithm. However, in the purely functional setting used in this work, the traditional approach to depth first search does not work well. Instead, this approach to spanning tree generation is defined in terms of mutual recursion between two functions: dfsInternal and processNeighbors.

dfsInternal is responsible for managing the high level collection and combination of subtrees created by exploring from some starting node $n$. Since $n$ may in general be any node in an already partially spanned graph, dfsInternal takes a description of the already visited nodes in this graph. It then passes this information along to a processNeighbors call for each of the children of $n$. As each of these computations returns, any subtrees are added to a local tree with $n$ as the root. In addition, each of these calls results in an updated description of the neighbors that has been added to the tree so far. 

processNeighbors examines a candidate neighbor position called by dfsInternal. At the time processNeighbors is called on some node $m$, there is no guarantee that this candidate neighbor to $n$ is in bounds or that it has not already been placed into the spanning tree that exists so far. Using the information passed down to it, processNeighbors simply performs these checks. If $m$ is in the relevant sets, it is added to the spanning tree as a child of $n$ by returning two parts to the instance of dfsInternal that called it. The first is a direct addition of $m$ and a subtree for $m$, obtained by another call to dfsInternal within processNeighbors, to the list of children of $n$ being built by the parent instance of dfsInternal. The second piece of information is the newly updated set of nodes that have been added to the tree so far. Passing this set up and down the chain of recursion is crucial to ensuring that each function call adds the appropriate collection of nodes to the spanning tree in this purely functional and recursive setting.

Because the neighbor function used to generate these spanning trees only looks for neighbors in cardinal directions at a distance determined by the custom scale of the problem (although usually either 2 or 6), not all environments and choices of root position are guaranteed to produce a spanning tree that contains every node in the environment. This is why it is necessary to generate spanning \textit{forests} for these environments. When one spanning tree fails to contain every coarse node with in bound locations in the environment, the set difference between the actual environment footprint and whatever nodes were added to the environment is calculated. This result is then passed to another call to the spanning tree generation function that starts with a new root. While it is possible and sometimes desirable to specify a custom root to the first spanning tree generated for an environment, these subsequent calls to generate additional trees simply use the minimum of the remaining positions as the new root. This process is repeated as necessary until a list of trees with all of the environment accounted for has been created. For most practical environments, which must be fully connected, it is often possible to generate a spanning forest that contains only a single spanning tree. The main exception to this is when a cell is only accessible through diagonal motion, and so most spanning forests contain one domainant tree and a small few subtrees. In the multi agent uses of spanning tree based algorithms, discussed later, there are additional reasons for a spanning forest as the footprints being spanned can be more irregular.

Once the spanning forest for an environment has been generated, this data can be used for multiple purposes. However, it is usually desirable to convert this spanning tree into the actual path that should be followed by a drone using this tree to guide its covering moves. The first step in providing this path is supplied by the cardinalCoveragePath function, which converts a sequence of positions to visit that, once completed, should consitute complete coverage of the environment at the appropriate hierarchical level. As mentioned earlier, this hierarchical level tends to group locations in the environment into either 2x2 or 6x6 squares. A 2x2 square contains four positions that must be visited at a low altitude in order to cover the entire square with high scrutiny. The 6x6 case is for high altitude drones, and a 6x6 square contains four 'quadrant center' positions that should each be visited in order to view the entire square at a low level of scrutiny. The cardinalCoveragePath function returns the overall sequence of locations to visit that will cause all nodes under the top level spanning forest to be covered. This function simply calls another function on each tree in the spanning forest and strings the results together at the end.

The function that generates a path for a single spanning tree has a little more going on. While the details are somewhat messy, this is another rescursive function. Briefly, this function keeps track of which 'quadrant center' it is on, and which direction is just came from. This gives the function enough information to decide which child of the current node it should attempt to visit next, or if it should generate the moves to get back to a new quadrant in its parent node and return to the higher level of recursion. While the overall path generated to go around a particular spanning tree is complex, it can be approximately described as counterclockwise motion.

Once this path has been generated for the entire spanning forest, it must be converted into a path that is realizable. This could be an issue in cases where the quadrant centers of a 6x6 square of positions are not guaranteed to be in bounds. To deal with these cases, out of bounds path locations are substituted with one or more waypoints which collectively provide enough vantage points to view every in bounds location within that quadrant. Then, the default version of A-Star search is used to find valid paths between each of these waypoints. This results in a sequence of adjacent environment positions that can be converted directly into the drone directions necessary to go around the total spanning forest.

\subsection{Details of the Low Spanning Tree Policy}

The Low Spanning Tree Policy (LSTP) uses the functions described above to generate a spanning forest for the environment it attempts to cover. As with other policies, it contains a map from drones to cached directions. In predictable settings with no drone failure, this map only needs to be filled once for the entire simulation to run. The spanning forest and subsequent path generation functions, bundled together in the same module where they are defined, is called for any drone in need of new directions. This gives the LSTP a sequence of adjacent positions that must be converted into the directions required to visit each of those positions. In addition to making the obvious mappings (i.e. the subsequence [(0, 0), (1, 1)] corresponds to an intercardinal motion to the northeast), moves must be supplied to get the drone from its current position to the start of its sequence of waypoints. This does not come up often for the LSTP, but it becomes an important consideration for the cases with multiple drones, drone failure, and other considerations that necessitate online replanning. Once these directions are in place, LSTP simply commands them in sequence like many other policies.

\section{The Clustering Low Spanning Tree Policy}

As the name suggests, the Clustering Low Spanning Tree Policy (CLSTP) modifies the approach taken by LSTP by introducing the same K Means Clustering insights used by the Clustering Low Policy. This allows for multiple drones to pursue different parts of the coverage task in parallel, as when K Means Clustering was introduced to the simpler low flying policy from Chapter 3.

The implementation of this policy is mostly a simple combination of the approaches used in LSTP and CLP. Specifically, the territory assigned to each drone is recomputed by one iteration of K Means Clustering at every step of the simulation, and each drone's directions are generated by creating a spanning tree of that drone's territory. However, it is not efficient to have each drone compute a new spanning tree of its ever changing territory at each time step. Not only would this use excessive computation, but the potential to derail each drone's progress by sending it to potentially different parts of a spanning tree would be very inefficient. In addition, the fact that a drone is likely to have many partially filled nodes in a spanning tree that it is in the middle of traversing means that a newly generated spanning forest path would command the drone to visit lots of already visited locations. While CLSTP includes functionality to discard already visited waypoints, the spanning tree paths that result would not have nearly the same efficiency as spanning tree paths generated for completely unexplored territory.

The opposite approach, in which each spanning tree is followed to completion before a drone does anything to acknowledge changes to its territory, works much better. This approach allows drones to take advantage of the reasonable first guess at territory division provided by the several iterations of K Means Clustering run at the beginning of each scenario. It also allows any drones that finish their assigned spanning trees before the others to move towards the sections of territory they need to take over. However, this variant of the CLSTP does not exhibit the same fluidity from repeated online strategy adaptation that was exhibited in CLP.

Surprisingly, a middle ground tends to work reasonably well: drones that cache roughly half of their assigned directions before considering changes to their spanning trees to a fairly good job at these tasks. It is not clear if or why this should be the case in general. However, it seems that the paths generated when following some spanning trees are amenable to being partially completed in a way that allows spanning trees over the rest of the environment to remain efficient. This could be because many of the same portions of the original spanning tree path remain intact for territories that have not changed too much, and ausing A-Star search to skip over large sections of already completed work allows the drone to resume its work on a slightly modified spanning tree with minimal introduction. If the spanning tree happens to be shaped in such a way that many entire branches of the tree have been completely traversed, such that there is no unfilled quadrant in any of that subtree's nodes, this approach works even better. The depth first search based approach to spanning tree generation tends to produce a relatively linear chain of subtrees, however, and is not particularly well suited to creating situations like this. Another approach to generating spanning trees, based on Breadth First Search, yields better results when applied to this strategy. This tree generation function is discussed shortly.

\subsection{The Adaptive Low Breadth First Search Policy}

The Adaptive Low Breadth First Search Policy (ALBP), uses a breadth first search based approach to spanning tree generation to create plans that are more easily adapted to opportunities for replanning that are discovered online.
This policy takes the best advantage of some of the opportunities for optimization as mentioned above.

The breadth first search approach to spanning tree creation yields a much cleaner implementation than the one used for depth first search. This has much to do with the simplicity of the queue used for selection of the next node to expand when performing breadth first search. In addition, representing spanning trees as a map from a node to its parent allows for functions that generate recursive data structures without recusrively descending into that data. This map is easily converted into the more explicit spanning tree representation used by depth first search. In turn, this allows for easy re-use of the path generation functions responsible for the directions that actually lead drones to traverse these spanning trees.

This use of breadth first search allows the ALBP to make spanning trees that are have a larger branching factor, and where the branches are more balanced, than with the trees generated by depth first search. This results in behavior where drones tend to cover sections of their assigned territory in more spatially distinct chunks. This also causes more entire nodes of the spanning tree to be covered early in the tree traversal process. As previously mentioned, this results in spanning trees that are more amenable to partial covering before territory changes lead to path modification. Finally, ALBP includes some checks to see when one drone's A-Star generated path to a waypoint happens to go through another drone's actual waypoint. This happens frequently in coverage scenarios, but it especially tends to occur at the very beginning of scenarios. This is because all drones start in the same location, and so some of them must fly through another drone's assigned space to get to their own.

\section{The High First Spanning Tree Policy}

One final policy combines three different approaches to achieve better coverage efficiency: hierarchical exploration, clustering based division of work, and spanning tree traversal based path planning. This is the High First Spanning Tree Policy (HSTP). When matched with the right environment, it is the most time efficient policy developed in this work. In accordance with the pattern established so far, it modifies the Adaptive Low Breadth First Search Policy (ALBP) in a completely analogous way to how the High Sweep First Policy modified the Low Sweep Policy.

The HSTP works by dividing the coverage task into a high altitude and a low altitude portion. The high altutude portion, unlike all other policies which use spanning trees, divides the environment into 6x6 nodes before computing its spanning trees. Pleasantly, HSTP is also like the other high altitude policy in that its high altitude requires very little in the way of specialized drone alignment code or other checks in the policy itself. Thanks to the relatively size-agnostic implementations of the various spanning tree functions, this policy's high altitude portion looks a lot like the ALBP implementation with a few key numbers enlarged and the altitudes swapped. In environments that are large enough to take full advantage of it, the efficient coverign behavior achieved by HSTP completes the high altitude portion of this policy in very little time.

During the low altitude phase of its coverage operations, HSTP simply refers to the policy instance used by ALBP, its low-only counterpart. Since both of these policies contain similar data, HSTP can simply pass along much of its internal contents transformed to fit the mold of a different datatype, then transform the resulting policy back to its own type and return the commanded moves. The effectiveness of this part of the policy is especially dependent on the environment that HSTP is covering. In environments with high scrutiny sections that appear in very small and frequent spots throughout the environment, this is not an effective strategy. The spanning tree approach to coverage helps very little when the resulting spanning forest contains many tiny trees, as the efficient within-tree motion of these algorithms means very little in trees with only one or two nodes. In addition, the paths used to route a drone between one tree and the next is not well studied or optimized in this work. As a result, the paths taken in situations like this can be erratic and obviously inefficient.

In environments with relatively infrequent and large groups of high scrutiny locations, however, this policy does very well. In environments like this, the job of clustering is very nearly done ahead of time, and K Means Clustering quickly converges to the obvious division of territory between drones. As each drone moves to handle a few of these naturally occuring clusters of high scrutiny locations, the efficient movement of the spanning tree based planning is on display for large stretches of time. As long as the overall fraction of patches requiring extra scrutiny is low in such an environment, HSTP's overall performance is among the best here. Though it is not justified with any rigor here, the intuition that most realiztic environments will contain only a few collected regions that are identifiable as worth inspecting in high detail makes these properties good news for the potential applicability of HSFP to a real world scenario.