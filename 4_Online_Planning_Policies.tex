\chapter{Intelligent and Multi Agent Coverage Policies}

The coverage policies described in the last chapter make use of A-Star, an algorithm often classified as an example of 'aritificial intelligence'. However, those policies attempt to work by following very simple rules most of the time, and A-Star is used only as a last resort. These policies are easy to understand due to their simplicity, and the adaptability of A-Star along with guarantees about properties of the environment make it easy to feel confident in their correctness. However, these policies may miss significant opportunities for behavior optimization in certain kinds of environment. In addition, the policies of the previous chapter did nothing to take advantage of the multi agent setting in which they operate. While the drone dropout and reintroduction feature introduced in this chapter will give these policies some redeeming properties for the multi agent case, more sophisticated approaches are necessary to take full advantage of the power of a team of covering robots.

%dropout makes the prev chapter policies multi agent

%Take out this section?
\section{Persistent (Learning) Policies}

In the last chapter, it was proved that the optimal policy with which to perform coverage depends on the distribution environments that policy will be asked to solve. In that chapter, this problem was approached by hand crafting policies to perform as well as possible on a complex distribution of environments through experimentation and tweaking of parameters. In this section, some policy implementations are developed that can recreate a version of this experimentation and tweaking across multiple scenarios. Importantly, these policies perform these tweaks \textit{automatically}, using the PersistentPolicy typeclass and associated environment interface tools to refine an approach specifically tuned to a distribution of environments encountered online.

It's worth addressing why this online learning behavior could be superior to a hand crafted approach. The distribution of real world environments in which you might want to deploy drone based robotic coverage is not at all easy to characterize. Furthermore, practical applications of this kind of  technology may see it deployed to various owners around the world. Any one of these teams of robots is likely to be deployed in environments with somewhat different properties the environments seen by other teams. As a result, there may be an opportunity to optimize the behavior of each of these teams of robots to the particular circumstances in which it is deployed. Such optimizations would not be easy to achieve through any sort of manual effort, so a well crafted policy that is capable of changing itself between individual operations may be the best way to achieve this optimization. In addition, there is some theoretical value in seeing how coverage policies can be automatically optimized for an experimentally determined distribution of environments. Much of the remainder of this chapter is devoted to exploring that topic.

%may need some introduction of online learning, perhaps see Billy's thesis for inspiration here