\chapter{Intelligent and Multi Agent Coverage Policies}

The coverage policies described in the last chapter make use of A-Star, an algorithm often classified as an example of 'artificial intelligence'. However, those policies attempt to work by following very simple rules most of the time, and A-Star is used only as a last resort. These policies are easy to understand due to their simplicity, and the adaptability of A-Star along with guarantees about properties of the environment make it easy to feel confident in their correctness. However, these policies may miss significant opportunities for behavior optimization in certain kinds of environment. In addition, the policies of the previous chapter did nothing to take advantage of the multi agent setting in which they operate. While the drone dropout and reintroduction feature introduced in this chapter will give these policies some redeeming properties for the multi agent case, more sophisticated approaches are necessary to take full advantage of the power of a team of covering robots.

\section{The Clustering Low Policy}
%K Means Low Policy in the code!

The first of these policies is the Clustering Low Policy (CLP). This policy manages a team of multiple drones by giving each of them a portion of the overall coverage space to complete. Because it is not straightforward to compute an optimal partitioning of an arbitrary coverage space between a team of covering robots, this policy uses an iterative clustering technique to come up with a reasonable approximation of this division. In addition, because this is the first of several policies designed to adapt to unforeseeable events such as drone dropout and reintroduction, this clustering operation is repeated throughout the coverage scenario. By adaptively redistributing assigned territory online, the CLP algorithm offers a single elegant solution to the dual problems of unforeseeable events and the difficulty of computing an optimal territory partition in one step.

\subsection{K Means Clustering}

The algorithm used to determine and change the division of territory assigned to each drone is a lightly modified variant of the K Means Clustering (KMC) algorithm. KMC was developed by \citeauthor{KMC} in 1957 and published in its standard form in 1982 \cite{KMC}. As it relates to this problem, the goal of KMC is to find some number $k$ of 'means' for a collection of data points such that the average squared distance from any point to the nearest mean is minimized. Once these means have been discovered, the original points can be divided into $k$ groups according to which of the $k$ means they are closest to.

The optimal solution to the problem of K Means Clustering is NP-Hard in general, so there is not a computationally efficient way to solve this problem perfectly. There are several algorithms which do a good job of approximating the solution to this problem with various claims to near-optimality and runtime. One of the most popular such algorithms and the one used in this work is the variant developed by \citeauthor{KMC}, sometimes known as 'naive k means'.

This variant of the algorithm is best known perhaps because of its simplicity. Given an initialization, possibly random, of $k$ means for some value of k, this algorithm simply repeats two steps. It assigns each point in the space to whichever mean is currently closest, then it replaces each mean with the centroid of the points assigned to it. This causes the location of the means to jump through the space with each iteration, gradually converging on a local minimum to the implicit cost function. It is common to simply run this algorithm until there is no change between two iterations, after which point the algorithm will make no further progress and terminates.

While Naive KMC is not the most computationally efficient way to perform K Means Clustering with a comparable level of optimality, it is more than adequate for this application. The number of points being divided into clusters tends to be small, and experimental runtime profiling suggests that these computations take a very small fraction of the overall time spent running simulations.

The exact implementation of K Means Clustering used in this work varies only slightly from the generic version described above. First, the mean of a collection of positions is always rounded into a discrete position value. This is because these means supply a possible location for the corresponding drone to be commanded to. In addition, because this variant of K Means Clustering will be run repeatedly on a smaller and smaller set of positions to be covered, it must eventually handle a case where some of the means actually have no positions associated with them. In this case, each drone gets a random uncovered position assigned to it. Finally, most runs of this K Means Variant use just one or a few iterations. This is because the previously described unconventional setting in which K Means CLustering is run can lead to issues with convergence. In addition, it is not necessary for a drone to have the optimally computed cluster before making any moves. Instead, four iterations of this algorithm are run at the start of most scenarios involving this or other policies that use clustering. This is enough to give each drone a distinct section of territory to pursue, and the particulars of the Clustering Low Policy, discussed below, ensure that this clustering algorithm continues to be used appropriately through the end of the scenario.

In other words, the use of k means clustering results in a partitioning of the space containing the data points into Voronoi cells. Because the algorithm penalizes clusters with large spatial extents quadratically, the Voronoi cells produced by K Means Clustering tend to be of comparable spatial extent to each other. Because of this property, using this algorithm to divide territory between drones tends to give each one a comparable amount of space to cover. This does not guarantee that each drone will require the same amount of time to cover its assigned territory, but it provides a reasonable first guess.

\subsection{Details of the Clustering Low Policy}

With the details of this customized K Means Clustering algorithm in place, it is now possible to describe the details of how the Clustering Low Policy commands moves. The CLP maintains a data structure that, among other things, associates a 'mean' position and a set of territory in the environment with each drone. Every time nextMove is called on an instance of this policy, these means and information about the environment information observed so far are passed to a function that sets up a new iteration of K Means Clustering. In this iteration, the set of locations to be divided and assigned to each mean is generated by reviewing which locations have not been completely covered.

Note that the details of this criteria can very in this project's other uses of the K Means Clustering algorithm. For example, some coverage policies that use KMC to command drones flying at a high altitude, discussed shortly, pass a function that will keep only completely unobserved locations in the set of points to be divided between means.

Once this information has been passed along, KMC returns a new set of means and territory assignments associated with each drone. Next, directions for each drone are computed. Each drone that is not already in the process of seeking a particular location is given a new location to seek. This location is selected from the most recently computed territory associated with the given drone. Within this set of possibilities, CLP attempts to select the location that is farthest from any other drone's mean position. Based on this assignment, a new sequence of movements is computed that will bring the drone from its current position to that newly assigned location. As in other policies, this sequence of moves uses A-Star search. The particular cost function used by A-Star applies a penalty to moves that would cause a drone to cover previously explored locations, and this penalty is especially effective at optimizing the paths created for the CLP.

The decision to make each drone seek out whatever position is farthest from all other means is a heuristic that attempts to have each drone explore the section of its territory that is most likely to be better suited to exploration by that drone than by any other. While this heuristic does not make optimal decisions with respect to that criteria, and while it is not clear that sequencing of positions to visit by such a heuristic will be useful, experimental results show that this policy works well compared to the simple sweep policies.

The rationale for assigning these 'middle-of-territory' positions to be visited first is that if another drone ends up needing more territory to explore, this heuristic ensures that the patches closest to that other drone are likely to go unexplored until that territory reassignment occurs. Likewise, if a drone exploring its own territory ends up needing to take on more work, the fact that other drones are behaving according to this heuristic makes it likely that more territory near where the drone is exploring will be available to take over. In addition, a well tuned choice of penalty function for A-Star path planning seems to handle most of the issues with how commanded locations are sequenced: many locations that should have been visited before the currently commanded one end up on the path generated by A-Star. Some of these descriptions of good behavior break down slightly as the number of locations to be covered dwindles to just a few per drone, but the bulk of the coverage task is handled well by this policy.

\section{A Single Agent Spanning Tree Policy}

As discussed in chapter one, many robotic coverage algorithms have managed to achieve good performance through methods that compute spanning trees on a modified graph of the environment. In this thesis, the Low Spanning Tree Policy (LSTP) is a policy that incorporates these improvements for the environments in \textit{oprc\_env}. LSTP, like the policies in chapter 3, is primarliy a single agent policy. While there are several more sophisticated multi agent spanning tree based policies developed for this setting, LSTP provides the most direct point of comparison to the other single agent policies. This makes it possible to analyze the effect of introducing the superior planning capabilities of this policy without having to consider the effects of the other policy changes. While these additional changes produce policies that tend to have faster overall coverage performance, the single agent policies tend to have some of the best results when it comes to the efficient utilization of individual drones. Before describing the Low Spanning Tree Policy itself, however, it is necessary to discuss the spanning tree generation algorithm that this and other policies rely on.

\subsection{Depth First Spanning Tree Generation and Path Generation Algorithms}

The spanning tree algorithm used by this policy generates a spanning forest, or a collection of spanning trees, using a depth first search based algorithm. However, in the purely functional setting used in this work, the traditional approach to depth first search does not work well. Instead, this approach to spanning tree generation is defined in terms of mutual recursion between two functions: dfsInternal and processNeighbors.

dfsInternal is responsible for managing the high level collection and combination of subtrees created by exploring from some starting node $n$. Since $n$ may in general be any node in an already partially spanned graph, dfsInternal takes a description of the already visited nodes in this graph. It then passes this information along to a processNeighbors call for each of the children of $n$. As each of these computations returns, any subtrees are added to a local tree with $n$ as the root. In addition, each of these calls results in an updated description of the neighbors that has been added to the tree so far. 

processNeighbors examines a candidate neighbor position called by dfsInternal. At the time processNeighbors is called on some node $m$, there is no guarantee that this candidate neighbor to $n$ is in bounds or that it has not already been placed into the spanning tree that exists so far. Using the information passed down to it, processNeighbors simply performs these checks. If $m$ is in the relevant sets, it is added to the spanning tree as a child of $n$ by returning two parts to the instance of dfsInternal that called it. The first is a direct addition of $m$ and a subtree for $m$, obtained by another call to dfsInternal within processNeighbors, to the list of children of $n$ being built by the parent instance of dfsInternal. The second piece of information is the newly updated set of nodes that have been added to the tree so far. Passing this set up and down the chain of recursion is crucial to ensuring that each function call adds the appropriate collection of nodes to the spanning tree in this purely functional and recursive setting.

Because the neighbor function used to generate these spanning trees only looks for neighbors in cardinal directions at a distance determined by the custom scale of the problem (although usually either 2 or 6), not all environments and choices of root position are guaranteed to produce a spanning tree that contains every node in the environment. This is why it is necessary to generate spanning \textit{forests} for these environments. When one spanning tree fails to contain every coarse node with in bound locations in the environment, the set difference between the actual environment footprint and whatever nodes were added to the environment is calculated. This result is then passed to another call to the spanning tree generation function that starts with a new root. While it is possible and sometimes desirable to specify a custom root to the first spanning tree generated for an environment, these subsequent calls to generate additional trees simply use the minimum of the remaining positions as the new root. This process is repeated as necessary until a list of trees with all of the environment accounted for has been created. For most practical environments, which must be fully connected, it is often possible to generate a spanning forest that contains only a single spanning tree. The main exception to this is when a cell is only accessible through diagonal motion, and so most spanning forests contain one domainant tree and a small few subtrees. In the multi agent uses of spanning tree based algorithms, discussed later, there are additional reasons for a spanning forest as the footprints being spanned can be more irregular.

Once the spanning forest for an environment has been generated, this data can be used for multiple purposes. However, it is usually desirable to convert this spanning tree into the actual path that should be followed by a drone using this tree to guide its covering moves. The first step in providing this path is supplied by the cardinalCoveragePath function, which converts a sequence of positions to visit that, once completed, should consitute complete coverage of the environment at the appropriate hierarchical level. As mentioned earlier, this hierarchical level tends to group locations in the environment into either 2x2 or 6x6 squares. A 2x2 square contains four positions that must be visited at a low altitude in order to cover the entire square with high scrutiny. The 6x6 case is for high altitude drones, and a 6x6 square contains four 'quadrant center' positions that should each be visited in order to view the entire square at a low level of scrutiny. The cardinalCoveragePath function returns the overall sequence of locations to visit that will cause all nodes under the top level spanning forest to be covered. This function simply calls another function on each tree in the spanning forest and strings the results together at the end.

The function that generates a path for a single spanning tree has a little more going on. While the details are somewhat messy, this is another rescursive function. Briefly, this function keeps track of which 'quadrant center' it is on, and which direction is just came from. This gives the function enough information to decide which child of the current node it should attempt to visit next, or if it should generate the moves to get back to a new quadrant in its parent node and return to the higher level of recursion. While the overall path generated to go around a particular spanning tree is complex, it can be approximately described as counterclockwise motion.

Once this path has been generated for the entire spanning forest, it must be converted into a path that is realizable. This could be an issue in cases where the quadrant centers of a 6x6 square of positions are not guaranteed to be in bounds. To deal with these cases, out of bounds path locations are substituted with one or more waypoints which collectively provide enough vantage points to view every in bounds location within that quadrant. Then, the default version of A-Star search is used to find valid paths between each of these waypoints. This results in a sequence of adjacent environment positions that can be converted directly into the drone directions necessary to go around the total spanning forest.

\subsection{Details of the Low Spanning Tree Policy}

The Low Spanning Tree Policy (LSTP) uses the functions described above to generate a spanning forest for the environment it attempts to cover. As with other policies, it contains a map from drones to cached directions. In predictable settings with no drone failure, this map only needs to be filled once for the entire simulation to run. The spanning forest and subsequent path generation functions, bundled together in the same module where they are defined, is called for any drone in need of new directions. This gives the LSTP a sequence of adjacent positions that must be converted into the directions required to visit each of those positions. In addition to making the obvious mappings (i.e. the subsequence [(0, 0), (1, 1)] corresponds to an intercardinal motion to the northeast), moves must be supplied to get the drone from its current position to the start of its sequence of waypoints. This does not come up often for the LSTP, but it becomes an important consideration for the cases with multiple drones, drone failure, and other considerations that necessitate online replanning. Once these directions are in place, LSTP simply commands them in sequence like many other policies.

\section{The Clustering Low Spanning Tree Policy}

As the name suggests, the Clustering Low Spanning Tree Policy (CLSTP) modifies the approach taken by LSTP by introducing the same K Means Clustering insights used by the Clustering Low Policy. This allows for multiple drones to pursue different parts of the coverage task in parallel, as when K Means Clustering was introduced to the simpler low flying policy from Chapter 3.

The implementation of this policy is mostly a simple combination of the approaches used in LSTP and CLP. Specifically, the territory assigned to each drone is recomputed by one iteration of K Means Clustering at every step of the simulation, and each drone's directions are generated by creating a spanning tree of that drone's territory. However, it is not efficient to have each drone compute a new spanning tree of its ever changing territory at each time step. Not only would this use excessive computation, but the potential to derail each drone's progress by sending it to potentially different parts of a spanning tree would be very inefficient. In addition, the fact that a drone is likely to have many partially filled nodes in a spanning tree that it is in the middle of traversing means that a newly generated spanning forest path would command the drone to visit lots of already visited locations. While CLSTP includes functionality to discard already visited waypoints, the spanning tree paths that result would not have nearly the same efficiency as spanning tree paths generated for completely unexplored territory.

The opposite approach, in which each spanning tree is followed to completion before a drone does anything to acknowledge changes to its territory, works much better. This approach allows drones to take advantage of the reasonable first guess at territory division provided by the several iterations of K Means Clustering run at the beginning of each scenario. It also allows any drones that finish their assigned spanning trees before the others to move towards the sections of territory they need to take over. However, this variant of the CLSTP does not exhibit the same fluidity from repeated online strategy adaptation that was exhibited in CLP.

Surprisingly, a middle ground tends to work reasonably well: drones that cache roughly half of their assigned directions before considering changes to their spanning trees to a fairly good job at these tasks. It is not clear if or why this should be the case in general. However, it seems that the paths generated when following some spanning trees are amenable to being partially completed in a way that allows spanning trees over the rest of the environment to remain efficient. This could be because many of the same portions of the original spanning tree path remain intact for territories that have not changed too much, and ausing A-Star search to skip over large sections of already completed work allows the drone to resume its work on a slightly modified spanning tree with minimal introduction. If the spanning tree happens to be shaped in such a way that many entire branches of the tree have been completely traversed, such that there is no unfilled quadrant in any of that subtree's nodes, this approach works even better. The depth first search based approach to spanning tree generation tends to produce a relatively linear chain of subtrees, however, and is not particularly well suited to creating situations like this. Another approach to generating spanning trees, based on Breadth First Search, yields better results when applied to this strategy. This tree generation function is discussed shortly.

\section{The Adaptive Low Breadth First Search Policy}

The Adaptive Low Breadth First Search Policy (ALBP), uses a breadth first search based approach to spanning tree generation to create plans that are more easily adapted to opportunities for replanning that are discovered online.

%dropout makes the prev chapter policies multi agent

%Take out this section?
\section{Persistent (Learning) Policies}

In the last chapter, it was proved that the optimal policy with which to perform coverage depends on the distribution environments that policy will be asked to solve. In that chapter, this problem was approached by hand crafting policies to perform as well as possible on a complex distribution of environments through experimentation and tweaking of parameters. In this section, some policy implementations are developed that can recreate a version of this experimentation and tweaking across multiple scenarios. Importantly, these policies perform these tweaks \textit{automatically}, using the PersistentPolicy typeclass and associated environment interface tools to refine an approach specifically tuned to a distribution of environments encountered online.

It's worth addressing why this online learning behavior could be superior to a hand crafted approach. The distribution of real world environments in which you might want to deploy drone based robotic coverage is not at all easy to characterize. Furthermore, practical applications of this kind of  technology may see it deployed to various owners around the world. Any one of these teams of robots is likely to be deployed in environments with somewhat different properties the environments seen by other teams. As a result, there may be an opportunity to optimize the behavior of each of these teams of robots to the particular circumstances in which it is deployed. Such optimizations would not be easy to achieve through any sort of manual effort, so a well crafted policy that is capable of changing itself between individual operations may be the best way to achieve this optimization. In addition, there is some theoretical value in seeing how coverage policies can be automatically optimized for an experimentally determined distribution of environments.