\chapter{Intelligent and Multi Agent Coverage Policies}

The coverage policies described in the last chapter make use of A-Star, an algorithm often classified as an example of 'artificial intelligence'. However, those policies attempt to work by following very simple rules most of the time, and A-Star is used only as a last resort. These policies are easy to understand due to their simplicity, and the adaptability of A-Star along with guarantees about properties of the environment make it easy to feel confident in their correctness. However, these policies may miss significant opportunities for behavior optimization in certain kinds of environment. In addition, the policies of the previous chapter did nothing to take advantage of the multi agent setting in which they operate. While the drone dropout and reintroduction feature introduced in this chapter will give these policies some redeeming properties for the multi agent case, more sophisticated approaches are necessary to take full advantage of the power of a team of covering robots.

\section{The Clustering Low Policy}
%K Means Low Policy in the code!

The first of these policies is the Clustering Low Policy (CLP). This policy manages a team of multiple drones by giving each of them a portion of the overall coverage space to complete. Because it is not straightforward to compute an optimal partitioning of an arbitrary coverage space between a team of covering robots, this policy uses an iterative clustering technique to come up with a reasonable approximation of this division. In addition, because this is the first of several policies designed to adapt to unforeseeable events such as drone dropout and reintroduction, this clustering operation is repeated throughout the coverage scenario. By adaptively redistributing assigned territory online, the CLP algorithm offers a single elegant solution to the dual problems of unforeseeable events and the difficulty of computing an optimal territory partition in one step.

\subsection{K Means Clustering}

The algorithm used to determine and change the division of territory assigned to each drone is a lightly modified variant of the K Means Clustering (KMC) algorithm. KMC was developed by \citeauthor{KMC} in 1957 and published in its standard form in 1982 \cite{KMC}. As it relates to this problem, the goal of KMC is to find some number $k$ of 'means' for a collection of data points such that the average squared distance from any point to the nearest mean is minimized. Once these means have been discovered, the original points can be divided into $k$ groups according to which of the $k$ means they are closest to.

The optimal solution to the problem of K Means Clustering is NP-Hard in general, so there is not a computationally efficient way to solve this problem perfectly. There are several algorithms which do a good job of approximating the solution to this problem with various claims to near-optimality and runtime. One of the most popular such algorithms and the one used in this work is the variant developed by \citeauthor{KMC}, sometimes known as 'naive k means'.

This variant of the algorithm is best known perhaps because of its simplicity. Given an initialization, possibly random, of $k$ means for some value of k, this algorithm simply repeats two steps. It assigns each point in the space to whichever mean is currently closest, then it replaces each mean with the centroid of the points assigned to it. This causes the location of the means to jump through the space with each iteration, gradually converging on a local minimum to the implicit cost function. It is common to simply run this algorithm until there is no change between two iterations, after which point the algorithm will make no further progress and terminates.

While Naive KMC is not the most computationally efficient way to perform K Means Clustering with a comparable level of optimality, it is more than adequate for this application. The number of points being divided into clusters tends to be small, and experimental runtime profiling suggests that these computations take a very small fraction of the overall time spent running simulations.

The exact implementation of K Means Clustering used in this work varies only slightly from the generic version described above. First, the mean of a collection of positions is always rounded into a discrete position value. This is because these means supply a possible location for the corresponding drone to be commanded to. In addition, because this variant of K Means Clustering will be run repeatedly on a smaller and smaller set of positions to be covered, it must eventually handle a case where some of the means actually have no positions associated with them. In this case, each drone gets a random uncovered position assigned to it. Finally, most runs of this K Means Variant use just one or a few iterations. This is because the previously described unconventional setting in which K Means CLustering is run can lead to issues with convergence. In addition, it is not necessary for a drone to have the optimally computed cluster before making any moves. Instead, four iterations of this algorithm are run at the start of most scenarios involving this or other policies that use clustering. This is enough to give each drone a distinct section of territory to pursue, and the particulars of the Clustering Low Policy, discussed below, ensure that this clustering algorithm continues to be used appropriately through the end of the scenario.

In other words, the use of k means clustering results in a partitioning of the space containing the data points into Voronoi cells. Because the algorithm penalizes clusters with large spatial extents quadratically, the Voronoi cells produced by K Means Clustering tend to be of comparable spatial extent to each other. Because of this property, using this algorithm to divide territory between drones tends to give each one a comparable amount of space to cover. This does not guarantee that each drone will require the same amount of time to cover its assigned territory, but it provides a reasonable first guess.

\subsection{Details of the Clustering Low Policy}

With the details of this customized K Means Clustering algorithm in place, it is now possible to describe the details of how the Clustering Low Policy commands moves. The CLP maintains a data structure that, among other things, associates a 'mean' position and a set of territory in the environment with each drone. Every time nextMove is called on an instance of this policy, these means and information about the environment information observed so far are passed to a function that sets up a new iteration of K Means Clustering. In this iteration, the set of locations to be divided and assigned to each mean is generated by reviewing which locations have not been completely covered.

Note that the details of this criteria can very in this project's other uses of the K Means Clustering algorithm. For example, some coverage policies that use KMC to command drones flying at a high altitude, discussed shortly, pass a function that will keep only completely unobserved locations in the set of points to be divided between means.

Once this information has been passed along, KMC returns a new set of means and territory assignments associated with each drone. Next, directions for each drone are computed. Each drone that is not already in the process of seeking a particular location is given a new location to seek. This location is selected from the most recently computed territory associated with the given drone. Within this set of possibilities, CLP attempts to select the location that is farthest from any other drone's mean position. Based on this assignment, a new sequence of movements is computed that will bring the drone from its current position to that newly assigned location. As in other policies, this sequence of moves uses A-Star search. The particular cost function used by A-Star applies a penalty to moves that would cause a drone to cover previously explored locations, and this penalty is especially effective at optimizing the paths created for the CLP.

The decision to make each drone seek out whatever position is farthest from all other means is a heuristic that attempts to have each drone explore the section of its territory that is most likely to be better suited to exploration by that drone than by any other. While this heuristic does not make optimal decisions with respect to that criteria, and while it is not clear that sequencing of positions to visit by such a heuristic will be useful, experimental results show that this policy works well compared to the simple sweep policies.

The rationale for assigning these 'middle-of-territory' positions to be visited first is that if another drone ends up needing more territory to explore, this heuristic ensures that the patches closest to that other drone are likely to go unexplored until that territory reassignment occurs. Likewise, if a drone exploring its own territory ends up needing to take on more work, the fact that other drones are behaving according to this heuristic makes it likely that more territory near where the drone is exploring will be available to take over. In addition, a well tuned choice of penalty function for A-Star path planning seems to handle most of the issues with how commanded locations are sequenced: many locations that should have been visited before the currently commanded one end up on the path generated by A-Star. Some of these descriptions of good behavior break down slightly as the number of locations to be covered dwindles to just a few per drone, but the bulk of the coverage task is handled well by this policy.

\section{The ??? Policy}

%dropout makes the prev chapter policies multi agent

%Take out this section?
\section{Persistent (Learning) Policies}

In the last chapter, it was proved that the optimal policy with which to perform coverage depends on the distribution environments that policy will be asked to solve. In that chapter, this problem was approached by hand crafting policies to perform as well as possible on a complex distribution of environments through experimentation and tweaking of parameters. In this section, some policy implementations are developed that can recreate a version of this experimentation and tweaking across multiple scenarios. Importantly, these policies perform these tweaks \textit{automatically}, using the PersistentPolicy typeclass and associated environment interface tools to refine an approach specifically tuned to a distribution of environments encountered online.

It's worth addressing why this online learning behavior could be superior to a hand crafted approach. The distribution of real world environments in which you might want to deploy drone based robotic coverage is not at all easy to characterize. Furthermore, practical applications of this kind of  technology may see it deployed to various owners around the world. Any one of these teams of robots is likely to be deployed in environments with somewhat different properties the environments seen by other teams. As a result, there may be an opportunity to optimize the behavior of each of these teams of robots to the particular circumstances in which it is deployed. Such optimizations would not be easy to achieve through any sort of manual effort, so a well crafted policy that is capable of changing itself between individual operations may be the best way to achieve this optimization. In addition, there is some theoretical value in seeing how coverage policies can be automatically optimized for an experimentally determined distribution of environments. Much of the remainder of this chapter is devoted to exploring that topic.

%may need some introduction of online learning, perhaps see Billy's thesis for inspiration here